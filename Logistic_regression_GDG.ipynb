{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4ab2cf3",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Linear regression is a machine learning model where it gives a relationship between the independent features and the dependent features in the form of a straight line. But here tthe output generated by the model is a discrete or categorical value. Also we have Z eff to derermine the loss function.\n",
    "\n",
    "loss --> loss can be defined as the difference between the original output and the model predicted output.\n",
    "\n",
    "loss function --> During the model training if we are passing a single datapoint from the dataset and the loss which we get is called the loss function\n",
    "\n",
    "cost function --> During the model training if we are passing the whole dataset, the loss which we get is called cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e440a3c",
   "metadata": {},
   "source": [
    "## Problem statement we are solving:-\n",
    "Implementing Logistic regression ML model using a imported dataset from sklearn library that is the iris dataset, where the model will get trained on different flower sepal length, sepal width, petal length,petal width and based on it it will give us respective flower with its desired specifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349308a8",
   "metadata": {},
   "source": [
    "## Importing necessary libraries for operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dfdabd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge, Lasso,  ElasticNet, LassoCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cf7c71",
   "metadata": {},
   "source": [
    "## Wrapping the dataset into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3418789a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fddde501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This are the attributes of the dataset\n",
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7908b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This are the data on which the model will get trained  \n",
    "iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8086710e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This are the output which will be predicted by them model once it get trained\n",
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d05bf56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(iris.data, columns = iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6187b2df",
   "metadata": {},
   "source": [
    "## Data preprocessing steps:\n",
    "1) data cleaning --> This is done to clean the noisy data, deal with inconsistent data, handling missing value and handling the outliers.\n",
    "Outliers == These are the unusual datapoints present in a dataset which does not follow the usual or predictive stats of a dataset \n",
    "\n",
    "2) data integration --> This is used to integrate multiple datas which are bought from different data sources to make a clear dataset.\n",
    "\n",
    "3) data selection --> This is used to select the data, on which we are keen to work and find the hidden patterns.\n",
    "\n",
    "4) data transformation --> This is done to scale down the values into a particular range so that the model training will be efficient.\n",
    "\n",
    "5) data reduction --> This is done to remove the less important or highly co related attributes.\n",
    "\n",
    "what excatly is the meaning of high co relation == For this we need to learn about the co relation, co relation gives us the strenght or how closely two attribute is related to each other, so a highly co related attribute means these attributes are highly related to each other and it will give a numeric value of nearer to 1, and for negatively co related it will tell how much it is not related to the other attribute. It will give value away from 1 i.e 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f341311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column:\n",
      "sepal length (cm)    0\n",
      "sepal width (cm)     0\n",
      "petal length (cm)    0\n",
      "petal width (cm)     0\n",
      "target               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Checking for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f3c8d0",
   "metadata": {},
   "source": [
    "## Current context of the working data\n",
    "\n",
    "In context of a dataset we are not having any outliers, redundant datas, inconsistent data, missing values so we are done with the data cleaning part\n",
    "\n",
    "Also the data which we have created is integrated already. So now integration step is done. We have to deal with this piece of data so the selection step is done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446734c6",
   "metadata": {},
   "source": [
    "## Spliting data\n",
    "we will split the dataset into two parts i.e the tain and test dataset, we will be using train-test split where we will perform 80-20 split, The 80% data will be used to train the data and 20% will be used to test the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4db3ad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.4, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26afc155",
   "metadata": {},
   "outputs": [],
   "source": [
    "## performing label encoding\n",
    "le = LabelEncoder()\n",
    "train_df['target'] = le.fit_transform(train_df['target'])\n",
    "test_df['target'] = le.transform(test_df['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dbed59",
   "metadata": {},
   "source": [
    "## Scaling the dataset\n",
    "Here we will use standard scaling technique to scale the dataset. Also we will separating the independent and dependent varaibales\n",
    "\n",
    "independent variables --> In a datset the data which is used to train the model is called the independent variables\n",
    "\n",
    "dependent variables --> In a datset the data which is used to predict as per the trained data is called the dependent variable or labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "001dcbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop('target',axis=1)\n",
    "y_train = train_df['target']\n",
    "X_test = test_df.drop('target',axis=1)\n",
    "y_test = test_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d9cfcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4fbb0d",
   "metadata": {},
   "source": [
    "## Model Training using Hyper parameter techniques\n",
    "Basic method --> Here we will be using the linear regression model for model training. While training the model we will be giving it X_train and y_train dataset.\n",
    "\n",
    "Hyperparameter method --> There are two types of hyper parameter techniques, i.e GridSearchCV and RandomizedSearchCV. To deal with bigger datasets we will be using the RandomizedSearchCV. or else it will be fine using the GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c54cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(multi_class='ovr', max_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74b75f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a8ccc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridSearchCV(estimator=model, param_grid=params, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4fd53a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_model = grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f035c3",
   "metadata": {},
   "source": [
    "## Model prediction\n",
    "For predicting the model we will be using the X_test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82b5ab9",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Model evaluation is a technique where we will be evaluating the model performance on the basis of the its predicted data and the y_test data. For this we are having different techniques such as MSE(Mean Squared Error), MAE(Mean Absolute Error), r2_score, RMSE(Root Mean Square Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3d08e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.33%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        25\n",
      "           1       0.95      1.00      0.97        19\n",
      "           2       1.00      0.94      0.97        16\n",
      "\n",
      "    accuracy                           0.98        60\n",
      "   macro avg       0.98      0.98      0.98        60\n",
      "weighted avg       0.98      0.98      0.98        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid.predict(X_test_scaled)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc*100:.2f}%\")\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cbcfb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved at: logistic_regression_model\\log_reg_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "model_dir = \"logistic_regression_model\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(model_dir, \"log_reg_model.pkl\")\n",
    "\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(grid_model, f)\n",
    "\n",
    "print(f\"✅ Model saved at: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643dbb70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
